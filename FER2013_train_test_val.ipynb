{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FER2013_train_test_val.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhuBn7VmwFEeUMwEfhzl0K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmswl0707/light_weighted_CNN_for_FER/blob/main/FER2013_train_test_val.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1oEw2td5cJD"
      },
      "source": [
        "import cv2\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from PIL import Image\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import torchvision.transforms as transforms\r\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\r\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\r\n",
        "from torchsummary import summary\r\n",
        "from torch import optim\r\n",
        "import copy\r\n",
        "import time\r\n",
        "import torch\r\n",
        "from torch.utils import data\r\n",
        "from torch import nn\r\n",
        "\r\n",
        "from torchvision.transforms import ToTensor\r\n",
        "from torchvision.datasets import ImageFolder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loE18RKl-asw"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "train_dir = '/content/drive/My Drive/Colab Notebooks/FER2013/train/'\r\n",
        "test_dir = '/content/drive/My Drive/Colab Notebooks/FER2013/test/'\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSnlNeLQlVDY"
      },
      "source": [
        "### Data augmentation\r\n",
        "\r\n",
        "transforms = transforms.Compose([transforms.Resize((48,48)),\r\n",
        "                                 transforms.Grayscale(),\r\n",
        "                                 transforms.RandomRotation(degrees=30),\r\n",
        "                                 transforms.RandomHorizontalFlip(),\r\n",
        "                                 transforms.ToTensor()\r\n",
        "                                 ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKQB4tyfkXm3"
      },
      "source": [
        "train_on_gpu = torch.cuda.is_available()\r\n",
        "print(train_on_gpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJLtuDl4rMII"
      },
      "source": [
        "### 데이터 불러오기\r\n",
        "\r\n",
        "df_train = ImageFolder(root= train_dir, transform = transforms)\r\n",
        "df_test = ImageFolder(root = test_dir, transform = transforms)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skm_wiA_Pn6j"
      },
      "source": [
        "# test 갯수와 카테고리 확인\r\n",
        "\r\n",
        "categories = list(df_test.class_to_idx.keys())\r\n",
        "print(categories)\r\n",
        "num_classes = len(categories)\r\n",
        "\r\n",
        "print(len(df_test))\r\n",
        "#print(len(num_classes))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfimrS6urM_L"
      },
      "source": [
        "#### train / test 데이터 데이터 구성확인\r\n",
        "\r\n",
        "def get_count(path,set_):\r\n",
        "    \r\n",
        "    dict_ = {}\r\n",
        "    \r\n",
        "    for exp in os.listdir(path):\r\n",
        "        dir_ = path + exp\r\n",
        "        dict_[exp] = len(os.listdir(dir_))\r\n",
        "        \r\n",
        "    df = pd.DataFrame(dict_,index=[set_])\r\n",
        "    return df\r\n",
        "    \r\n",
        "\r\n",
        "train_count = get_count(train_dir,'train')\r\n",
        "test_count = get_count(test_dir,'test')\r\n",
        "\r\n",
        "print(train_count)\r\n",
        "print(test_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcWvy0NHrUr4"
      },
      "source": [
        "#train, test set의 class 분포 확인\r\n",
        "'''\r\n",
        "emotions = {0 : 'angry', 1: 'disgust', 2 : 'fear', 3 : 'happy', 4 : 'sad', 5 : 'surprise', 6: 'neutral'}\r\n",
        "\r\n",
        "em_count = df_train['emotion'].value_counts().plot(kind='bar')\r\n",
        "plt.xlabel(\"total\", labelpad=14)\r\n",
        "plt.ylabel(\"emotions\", labelpad=14)\r\n",
        "plt.title(\"value_counts by emotions_[train]\", y=1.02);\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1BxVgavqFaq"
      },
      "source": [
        "### train /val split\r\n",
        "\r\n",
        "val_size = 0.25 # train set의 0.2 사이즈 만큼\r\n",
        "\r\n",
        "num_data = len(df_train)\r\n",
        "indices = list(range(num_data))\r\n",
        "np.random.shuffle(indices)\r\n",
        "\r\n",
        "\r\n",
        "val_split = int(np.floor(val_size * num_data))\r\n",
        "val_idx, train_idx = indices[:val_split], indices[val_split:]\r\n",
        "\r\n",
        "\r\n",
        "print(len(train_idx), len(val_idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaej8U9wGwUg"
      },
      "source": [
        "\r\n",
        "###데이터 로더\r\n",
        "\r\n",
        "\r\n",
        "train_sampler = SubsetRandomSampler(train_idx)\r\n",
        "val_sampler = SubsetRandomSampler(val_idx)\r\n",
        "test_sampler = SubsetRandomSampler(df_test)\r\n",
        "\r\n",
        "batch_size=16\r\n",
        "\r\n",
        "loaders = {\r\n",
        "    'train': torch.utils.data.DataLoader(train_idx, batch_size=batch_size, shuffle=True, num_workers=2),\r\n",
        "    'val': torch.utils.data.DataLoader(val_idx, batch_size=batch_size, shuffle=True, num_workers=2)\r\n",
        "}\r\n",
        "test_loader = torch.utils.data.DataLoader(df_test, batch_size=batch_size, shuffle=True, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXr_8VTtr4Li"
      },
      "source": [
        "## 데이터 확인하기\r\n",
        "\r\n",
        "def imshow(img):\r\n",
        "    img = (img+1)/2    \r\n",
        "    img = img.squeeze()\r\n",
        "    np_img = np.asarray(img)\r\n",
        "    plt.imshow(np_img)\r\n",
        "\r\n",
        "\r\n",
        "dataiter = iter(loaders['train'])\r\n",
        "images, labels = dataiter.next()\r\n",
        "print(images.shape,labels.shape)\r\n",
        "images = images.numpy() \r\n",
        "fig = plt.figure(figsize=(25, 10))\r\n",
        "\r\n",
        "for idx in np.arange(8):\r\n",
        "    ax = fig.add_subplot(2, 4, idx+1, xticks=[], yticks=[])\r\n",
        "    imshow(images[idx])\r\n",
        "    ax.set_title(categories[int(labels[idx])],fontsize=20,color='white')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn2NtBO0sAj7"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.nn.utils.prune as prune\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "\r\n",
        "class SeparableConv2d(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False):\r\n",
        "        super(SeparableConv2d, self).__init__()\r\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels,\r\n",
        "                                   bias=bias)\r\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, 1, 0, 1, 1, bias=bias)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.depthwise(x)\r\n",
        "        x = self.pointwise(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class ResidualBlock(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, in_channeld, out_channels):\r\n",
        "        super(ResidualBlock, self).__init__()\r\n",
        "\r\n",
        "        self.residual_conv = nn.Conv2d(in_channels=in_channeld, out_channels=out_channels, kernel_size=1, stride=2,\r\n",
        "                                       bias=False)\r\n",
        "        self.residual_bn = nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\r\n",
        "\r\n",
        "        self.sepConv1 = SeparableConv2d(in_channels=in_channeld, out_channels=out_channels, kernel_size=3, bias=False,\r\n",
        "                                        padding=1)\r\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\r\n",
        "        self.elu = nn.ELU()\r\n",
        "\r\n",
        "        self.sepConv2 = SeparableConv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, bias=False,\r\n",
        "                                        padding=1)\r\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\r\n",
        "        self.maxp = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        res = self.residual_conv(x)\r\n",
        "        res = self.residual_bn(res)\r\n",
        "        x = self.sepConv1(x)\r\n",
        "        x = self.bn1(x)\r\n",
        "        x = self.elu(x)\r\n",
        "        x = self.sepConv2(x)\r\n",
        "        x = self.bn2(x)\r\n",
        "        x = self.maxp(x)\r\n",
        "        return res + x\r\n",
        "\r\n",
        "\r\n",
        "class Block(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, in_channeld, out_channels):\r\n",
        "        super(Block, self).__init__()\r\n",
        "\r\n",
        "        self.sep1 = SeparableConv2d(in_channels=in_channeld, out_channels=out_channels, kernel_size=3, bias=False, padding=1)\r\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\r\n",
        "        self.elu1 = nn.ELU()\r\n",
        "        self.sep2 = SeparableConv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, bias=False, padding=1)\r\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\r\n",
        "        self.elu2 = nn.ELU()\r\n",
        "        self.maxp = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.sep1(x)\r\n",
        "        x = self.bn1(x)\r\n",
        "        x = self.elu1(x)\r\n",
        "\r\n",
        "        x = self.sep2(x)\r\n",
        "        x = self.bn2(x)\r\n",
        "        x = self.elu2(x)\r\n",
        "        x = self.maxp(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class SELayer(nn.Module):\r\n",
        "    def __init__(self, in_channeld, reduction=16):\r\n",
        "        super(SELayer, self).__init__()\r\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\r\n",
        "        self.fc = nn.Sequential(\r\n",
        "            nn.Linear(in_channeld, in_channeld // reduction, bias=False),\r\n",
        "            nn.ReLU6(inplace=True),\r\n",
        "            nn.Linear(in_channeld // reduction, in_channeld, bias=False),\r\n",
        "            nn.Sigmoid()\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        b, c, _, _ = x.size()\r\n",
        "        y = self.avg_pool(x).view(b, c)\r\n",
        "        y = self.fc(y).view(b, c, 1, 1)\r\n",
        "        return x * y.expand_as(x)\r\n",
        "\r\n",
        "\r\n",
        "class SEBasicBlock(nn.Module):\r\n",
        "    expansion = 1\r\n",
        "\r\n",
        "    def __init__(self, in_channeld, out_channels, stride=1, downsample=None, groups=1,\r\n",
        "                 base_width=64, dilation=1, norm_layer=None,\r\n",
        "                 *, reduction=16):\r\n",
        "        super(SEBasicBlock, self).__init__()\r\n",
        "        self.Conv1 = nn.Conv2d(in_channeld, out_channels, stride)\r\n",
        "        self.bn1 = nn.BatchNorm2d(in_channeld)\r\n",
        "        self.relu = nn.ReLU6(inplace=True)\r\n",
        "\r\n",
        "        self.Conv2 = nn.Conv2d(in_channeld, out_channels, 1)\r\n",
        "        self.bn2 = nn.BatchNorm2d(in_channeld)\r\n",
        "        self.se = SELayer(in_channeld, reduction)\r\n",
        "        self.downsample = downsample\r\n",
        "        self.stride = stride\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        residual = x\r\n",
        "        out = self.Conv1(x)\r\n",
        "        out = self.bn1(out)\r\n",
        "        out = self.relu(out)\r\n",
        "        out = self.Conv2(out)\r\n",
        "        out = self.bn2(out)\r\n",
        "        out = self.se(out)\r\n",
        "\r\n",
        "        if self.downsample is not None:\r\n",
        "            residual = self.downsample(x)\r\n",
        "\r\n",
        "        out += residual\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "class Model(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, num_classes):\r\n",
        "        super(Model, self).__init__()\r\n",
        "\r\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, dilation=2, bias=False)\r\n",
        "        self.bn1 = nn.BatchNorm2d(8, affine=True, momentum=0.99, eps=1e-3)\r\n",
        "        self.relu1 = nn.ELU()\r\n",
        "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, dilation=2, bias=False)\r\n",
        "        self.bn2 = nn.BatchNorm2d(8, momentum=0.99, eps=1e-3)\r\n",
        "        self.relu2 = nn.ELU()\r\n",
        "\r\n",
        "\r\n",
        "        self.module1 = ResidualBlock(in_channeld=8, out_channels=16)\r\n",
        "        self.module2 = SEBasicBlock(16, 16, stride=1, downsample=None, groups=1,\r\n",
        "                                    base_width=64, dilation=1, norm_layer=None, reduction=16)\r\n",
        "        self.module3 = Block(in_channeld=16, out_channels=32)\r\n",
        "        self.module4 = ResidualBlock(in_channeld=32, out_channels=64)\r\n",
        "        self.module5 = SEBasicBlock(64, 64, stride=1, downsample=None, groups=1,\r\n",
        "                                    base_width=64, dilation=1, norm_layer=None, reduction=16)\r\n",
        "        self.module6 = Block(in_channeld=64, out_channels=128)\r\n",
        "\r\n",
        "\r\n",
        "        self.last_conv = nn.Conv2d(in_channels=128, out_channels=num_classes, kernel_size=3, padding=1, stride=2)\r\n",
        "        self.avgp = nn.AdaptiveAvgPool2d((1, 1))\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        x = input\r\n",
        "        x = self.conv1(x)\r\n",
        "        x = self.bn1(x)\r\n",
        "        x = self.relu1(x)\r\n",
        "        x = self.conv2(x)\r\n",
        "        x = self.bn2(x)\r\n",
        "        x = self.relu2(x)\r\n",
        "        x = self.module1(x)\r\n",
        "        x = self.module2(x)\r\n",
        "        x = self.module3(x)\r\n",
        "        x = self.module4(x)\r\n",
        "        x = self.module5(x)\r\n",
        "        x = self.module6(x)\r\n",
        "        x = self.last_conv(x)\r\n",
        "        x = self.avgp(x)\r\n",
        "        x = x.view((x.shape[0], -1))\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcFvjI8EWPs_"
      },
      "source": [
        "def graph_loss(train_loss,test_loss):\r\n",
        "    fig,ax = plt.subplots(1,1,figsize=(5,5))\r\n",
        "    ax.plot(train_loss)\r\n",
        "    ax.plot(val_loss)\r\n",
        "    ax.legend(['Train Loss','val Loss'])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSP0HJwbW3bD"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMp2nBFUU9zI"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\r\n",
        "    train_loss, val_loss = [], []\r\n",
        "    since = time.time()\r\n",
        "\r\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\r\n",
        "    best_acc = 0.0\r\n",
        "\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\r\n",
        "        print('-' * 10)\r\n",
        "\r\n",
        "        # Each epoch has a training and validation phase\r\n",
        "        for phase in ['train', 'val']:\r\n",
        "            if phase == 'train':\r\n",
        "                model.train()  # Set model to training mode\r\n",
        "            if phase == 'val':\r\n",
        "                model.eval()  # Set model to evaluate mode\r\n",
        "\r\n",
        "            running_loss = 0.0\r\n",
        "            running_corrects = 0\r\n",
        "\r\n",
        "            # Iterate over data.\r\n",
        "            for inputs, labels in loaders[phase]:\r\n",
        "                inputs = inputs.to(device)\r\n",
        "                labels = labels.to(device)\r\n",
        "\r\n",
        "                # zero the parameter gradients\r\n",
        "                optimizer.zero_grad()\r\n",
        "\r\n",
        "                # forward\r\n",
        "                # track history if only in train\r\n",
        "                with torch.set_grad_enabled(phase == 'train'):\r\n",
        "                    outputs = model(inputs)\r\n",
        "                    _, preds = torch.max(outputs, 1)\r\n",
        "                    loss = criterion(outputs, labels)\r\n",
        "\r\n",
        "                    # backward + optimize only if in training phase\r\n",
        "                    if phase == 'train':\r\n",
        "                        loss.backward()\r\n",
        "                        optimizer.step()\r\n",
        "\r\n",
        "                # statistics\r\n",
        "                running_loss += loss.item() * inputs.size(0)\r\n",
        "                running_corrects += torch.sum(preds == labels.data)\r\n",
        "\r\n",
        "            if phase == 'train':\r\n",
        "                scheduler.step()\r\n",
        "\r\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\r\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\r\n",
        "\r\n",
        "            if phase == 'train':\r\n",
        "                train_loss.append(epoch_loss)\r\n",
        "                writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\r\n",
        "                writer.add_scalar(\"loss/train\", epoch_loss, epoch)\r\n",
        "\r\n",
        "            if phase == 'val' :\r\n",
        "                test_loss.append(epoch_loss)\r\n",
        "                writer.add_scalar(\"Acc/test\", epoch_acc, epoch)\r\n",
        "                writer.add_scalar(\"loss/test\", epoch_loss, epoch)\r\n",
        "\r\n",
        "\r\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\r\n",
        "                phase, epoch_loss, epoch_acc))\r\n",
        "\r\n",
        "\r\n",
        "            # deep copy the model\r\n",
        "            if phase == 'val' and epoch_acc > best_acc:\r\n",
        "                best_acc = epoch_acc\r\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        print()\r\n",
        "\r\n",
        "    time_elapsed = time.time() - since\r\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\r\n",
        "        time_elapsed // 60, time_elapsed % 60))\r\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\r\n",
        "\r\n",
        "    graph_loss(train_loss, test_loss)\r\n",
        "\r\n",
        "\r\n",
        "    # load best model weights\r\n",
        "    model.load_state_dict(best_model_wts)\r\n",
        "    return model\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G22ZoKLVs-A"
      },
      "source": [
        "def visualize_model(model, num_images=6):\r\n",
        "    was_training = model.training\r\n",
        "    model.eval()\r\n",
        "    images_so_far = 0\r\n",
        "    fig = plt.figure()\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        for i, (inputs, labels) in enumerate(test_loader):\r\n",
        "            inputs = inputs.to(device)\r\n",
        "            labels = labels.to(device)\r\n",
        "\r\n",
        "            outputs = model(inputs)\r\n",
        "            _, preds = torch.max(outputs, 1)\r\n",
        "            fig = plt.figure(figsize=(25, 16))\r\n",
        "\r\n",
        "            for j in range(inputs.size()[0]):\r\n",
        "                images_so_far += 1\r\n",
        "                ax = fig.add_subplot(2, num_images//2, images_so_far)\r\n",
        "                ax.axis('off')\r\n",
        "                ax.set_title('predicted: {}'.format(class_labels[preds[j]]))\r\n",
        "                imshow(inputs.cpu().data[j])\r\n",
        "\r\n",
        "                if images_so_far == num_images:\r\n",
        "                    model.train(mode=was_training)\r\n",
        "                    return\r\n",
        "        model.train(mode=was_training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i9-nt5eWlgp"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPFw_8GnWn5g"
      },
      "source": [
        "model_ft = Model(num_classes=7)\r\n",
        "model_ft = model_ft.to(device)\r\n",
        "print(summary(model_ft,(1,44,44)))\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), momentum=0.9,lr=0.02, weight_decay=5e-3)\r\n",
        "\r\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\r\n",
        "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYEIZ7rtWob9"
      },
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=300)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}